{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is one of the excellent [UCI datasets](https://archive.ics.uci.edu/ml/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = [line.rstrip() for line in open('SMSSpamCollection')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many messages are contained in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an arbitrary message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam\\t07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about looking at a few more at once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, msg in enumerate(sms[:10]):\n",
    "    print(f'{idx} {msg}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is a CSV file, but rather than commas, tabs are used as separators, in conflict with the name of the extension, but whatever. From what you've seen you may have also figured out what we're going to be doing here: we'll be trying to discern spam from ham based on text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', names=['label', 'message'])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `DataFrame.describe()` can show us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's neat. I'm sure that top message value comes from automated response messages sent while the user is making a phone call or is just busy. Since we're interested in the labels, let's try repeating the above after first grouping by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to start thinking about what features we are going to use in this machine learning problem. Text, especially this text, is messier than simple numbers and categories, so the feature engineering step is going to much more involved this time.\n",
    "\n",
    "One feature that might be useful is simply message length. Let's make a column for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms = sms.assign(length = sms['message'].apply(len))\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we see at a glance if the average message length of a spam message os different from a ham message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>71.482487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>138.670683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           length\n",
       "label            \n",
       "ham     71.482487\n",
       "spam   138.670683"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.groupby('label').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a difference! What are the overall descriptive stats on this new column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard to believe that anyone sent a message with 910 characters. Can we see what that message was?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms['length'] == sms['length'].max()]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How romantic. I wonder where those two are now?\n",
    "\n",
    "Let's visualize this new feature. Recall that pandas has some built-in plotting tools. These tools can even mimic some of the cool things that Seaborn's factor plots can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "\n",
    "sms.hist(column='length', by='label', bins='doane', figsize=(15, 5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even taking into consideration the different scales of the $x$ axes, you can see the difference between the two types of messages. Clearly spam messages tend to be longer than ham messages, love letter notwithstanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing\n",
    "\n",
    "We can't do much the text data as-is, although getting the message length was a good start. Machine learning algorithms demand that we transform these strings into numerical vectors. Ther're a few ways to do this, but we'll be using one of the simpler and more common ones: the *bag of words*.\n",
    "\n",
    "We're about to get into the nitty-gritty of text processing by splitting, joining, and mucking about with lists. It's all inteded to be a learning exercise. In one's day job, there are sophisticated language processing libraries that take care of most of this.\n",
    "\n",
    "Let's start by importing the `string` library and making a sample message to work with. We'll use our tinkering to build a function that we'll latter `apply` to each message in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "msg = 'This is sample message. Do you see any punctuation? Of course you see punctuation!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library provides a list of ASCII punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's strip all of the punctuation from that message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is sample message Do you see any punctuation Of course you see punctuation'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = ''.join(char for char in msg if char not in string.punctuation)\n",
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the `nltk` library and use it to strip any stopwords. You might have to download some additional files through the library's interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'sample',\n",
       " 'message',\n",
       " 'Do',\n",
       " 'see',\n",
       " 'punctuation',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'see',\n",
       " 'punctuation']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "msg_sans_sw = [word for word in msg.split() if word not in eng_stopwords]\n",
    "msg_sans_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the first few steps of basic text processing. Like I said, there are libraries that can take of this stuff for you, but it's beneficial to do it by hand until you're familiar with the logic behind it.\n",
    "\n",
    "Before venturing much deeper, let's formalize the above few steps into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(line):\n",
    "    \"\"\"\n",
    "    Prepare a line of text for machine learning algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : string\n",
    "        A string of natural language, ideally space-separated.\n",
    "    stopwords : array_like\n",
    "        An array of stopwords to be removed from the line.\n",
    "    punctuation : string, array_like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sans_stops : list\n",
    "        A list of words from the text, minus punctuation and common stopwords.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    global stop\n",
    "    global punc\n",
    "    \n",
    "    sans_punct = ''.join([char for char in line if char not in punc])\n",
    "    sans_stops = [word for word in sans_punct.split() if word not in stop]\n",
    "    return sans_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'GREAT', 'sample', 'text', '123']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = 'This is a GREAT sample text, 123?'\n",
    "stop = stopwords.words('english')\n",
    "punc = string.punctuation\n",
    "\n",
    "process(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It's important to note that this function does not normalize letter case. Other text processing methods will often convert everything to lower case if that is seen as an irrelevant feature, but when it comes to spam detection, I surmise that case is acutally very important. To make my point, consider a message that starts with \"FREE\" (in all-caps). That word, especially because it is in all-caps, just screams spam, doesn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Now we can transform those text messages into a vector format that machine learning algorithms can understand. We'll use the *bag of words* method mentioned before. It works like this:\n",
    "\n",
    "1. Count how many times a word occurs in each message (term frequency).\n",
    "1. Weigh the counts, so that freqent words get lower weight (inverse document frequency).\n",
    "1. Normaliza the vectors to abstract from text length (L2 norm).\n",
    "\n",
    "We'll use Scikit-Learn's `CountVectorizer` which is able to take a collection of texts as input and return a two-dimensional matrix of token counts. One dimension will comprise the entire vocabulary (the rows in this case) while the individual documents are represented by the other dimension (the columns). It'll look something like this:\n",
    "\n",
    "|                  | message_1 | message_2| ... | message_n |\n",
    "| ---------------- | --------- | -------- | --- | --------- |\n",
    "| word_1_frequency |         0 |        2 |     |         1 |\n",
    "| word_1_frequency |         0 |        1 |     |         0 |\n",
    "|              ... |       ... |      ... | ... |       ... |\n",
    "| word_n_frequency |         1 |        0 |     |         0 |\n",
    "\n",
    "Not surprisingly this kind of matrix is going to have a looooooot of zeroes. Fortunately, Scikit-Learn is able to implement various kinds of sparse matricies that alleviate the need to watse memory space on giant swaths of nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_transformer = CountVectorizer(lowercase=False, analyzer=process).fit(sms['message'])\n",
    "type(bag_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not quite accurate, you can think of this `bag_transformer` as like a design for an empty bag, with many pockets therein that will hold counts of specific words. We'll use this  to `transform()` messages which will produce a complete bag of words.\n",
    "\n",
    "It actually wasn't necessary to specify `lowercase=False` in the above call, because specifying your own analyzer causes that particular argument to be ignored (as well as some others). I just wanted you to acknowledge that it exists, and that we definitely do want a case-sensitive bag o' words.\n",
    "\n",
    "So how many unique words are in this bag anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11617"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bag_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's demonstrate the power of this fully-operational vectorizer on a single message. What message resides at index locations 42?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sms = sms['message'][42]\n",
    "sample_sms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's put it through our vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(1, 11617)\n",
      "  (0, 29)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 1363)\t1\n",
      "  (0, 2846)\t1\n",
      "  (0, 3356)\t1\n",
      "  (0, 3552)\t1\n",
      "  (0, 4456)\t1\n",
      "  (0, 5467)\t2\n",
      "  (0, 5494)\t1\n",
      "  (0, 6142)\t1\n",
      "  (0, 6891)\t2\n",
      "  (0, 8408)\t1\n",
      "  (0, 8647)\t1\n",
      "  (0, 9541)\t1\n",
      "  (0, 10068)\t1\n",
      "  (0, 10741)\t1\n",
      "  (0, 10830)\t1\n"
     ]
    }
   ],
   "source": [
    "sample_bag = bag_transformer.transform([sample_sms])\n",
    "\n",
    "print(type(sample_bag))\n",
    "print(sample_bag.shape)\n",
    "print(sample_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the words back out of the bag like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07732584351\n",
      "08000930705\n",
      "Burns\n",
      "MSG\n",
      "Please\n",
      "Rodger\n",
      "We\n",
      "call\n",
      "camcorder\n",
      "delivery\n",
      "free\n",
      "mobile\n",
      "nokia\n",
      "reply\n",
      "sms\n",
      "tomorrow\n",
      "tried\n"
     ]
    }
   ],
   "source": [
    "for w in sample_bag.nonzero()[1]:\n",
    "    print(bag_transformer.get_feature_names()[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did with that sample message, we can now `transform()` the entire collection of messages. This might take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_bag = bag_transformer.transform(sms['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some descriptives of the sparse matrix we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix Shape: (5572, 11617)\n",
      "Non-zero Occurences: 56024\n",
      "Sparsity: 0.09\n"
     ]
    }
   ],
   "source": [
    "print(f'Sparse Matrix Shape: {sms_bag.shape}')\n",
    "print(f'Non-zero Occurences: {sms_bag.nnz}')\n",
    "\n",
    "sparsity = (100.0 * sms_bag.nnz) / (sms_bag.shape[0] * sms_bag.shape[1])\n",
    "print(f'Sparsity: {sparsity:1.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the counting is finished, weighting and normalization of terms can be accomplished by TD-IDF, which Scikit-Learn does with its `TfidTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TD-IDF stands for *term frequency-inverse document frequency*. Its weighting is often used in information retrieval and text mining. It is a statistical measure of how important a word is within a document, when that document is also part of a larger corpus. The importance of a word increases with its frequency in a document, but that is balanced by the frequency of word in the whole corpus, which lowers the importance as it occurs. Search engines make use of this when trying to judge how relevant a document is to a query.\n",
    "\n",
    "Typically two terms are involved in calculating TF-IDF. The first is the normalized term frequency (TF) and the second is inverse document frequency (IDF).\n",
    "\n",
    "**Term frequency (TF)** is the number of times a term appears in a document divided by the total number of terms in that document.\n",
    "\n",
    "**Inverse document frequency (IDF)** balances term frequency (which assumes equal importance for all terms) by comparing the number of documents in a corpus with the number of documents that contain the term. Both of those terms are under a log_e.'\n",
    "\n",
    "**Term frequency-inverse document frequency (TF-IDF)** is the product of TF and IDF.\n",
    "\n",
    "#### Example\n",
    "\n",
    "You have a **500** word document that includes the word ***fnord*** **23** times. It is part of a corpus that contains **10,000** documents, of which **7** documents (including the mentioned just above) contain that word. Here is how you would calculate the term frequency of that term, in that document.\n",
    "\n",
    "$$\n",
    "\\large\n",
    "\\text{term frequency} = 23 \\div 500 = 0.046\n",
    "\\\\\n",
    "\\large\n",
    "\\text{inverse document frequency} = \\log (10,000 \\div 7) = 4.9618\n",
    "\\\\\n",
    "\\large\n",
    "\\text{tf-idf} = 0.046 \\times 4.9618 = \\color{red}{0.0093}\n",
    "$$\n",
    "\n",
    "Let's see how this is done with Scikit-Learn using that sample bag we made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10830)\t0.21825402097992086\n",
      "  (0, 10741)\t0.17509730090883832\n",
      "  (0, 10068)\t0.22649706539217168\n",
      "  (0, 9541)\t0.1731301746263505\n",
      "  (0, 8647)\t0.2582576761897893\n",
      "  (0, 8408)\t0.16719063052852873\n",
      "  (0, 6891)\t0.3318667049465819\n",
      "  (0, 6142)\t0.23045790521776877\n",
      "  (0, 5494)\t0.2537672463504903\n",
      "  (0, 5467)\t0.24985079842590838\n",
      "  (0, 4456)\t0.1641314069305411\n",
      "  (0, 3552)\t0.30038592861593333\n",
      "  (0, 3356)\t0.18229872746676004\n",
      "  (0, 2846)\t0.26957265569927513\n",
      "  (0, 1363)\t0.30038592861593333\n",
      "  (0, 54)\t0.2284192040146275\n",
      "  (0, 29)\t0.30038592861593333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(sms_bag)\n",
    "sample_tfidf = tfidf_transformer.transform(sample_bag)\n",
    "print(sample_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's neat. How about some other words from the whole set?\n",
    "\n",
    "Here we'll find the IDF of the word *university* in the SMS dataset. The code looks a little funky, so I'll explain it.\n",
    "\n",
    "The object `tfidf_transformer` is an instance of an `TfidfTransformer`. It has a property called `idf_` which contains the inverse document frequency of every word in the count matrix that has been fit to it, which the SMS dataset bag of words in this case. The bag transformer has a property called `vocabulary_` which is a dictionary wherein the keys are the words and the values are the indices of the words in the document-term matrix (I think). So the code below will return the IDF value at the index of the word *university* as found in the bag_transformer vocabulary dictionary. Hope that was at least minimally clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.527076498901426"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer.idf_[bag_transformer.vocabulary_['university']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 11617)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_tfidf = tfidf_transformer.transform(sms_bag)\n",
    "sms_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just one of many ways to vectorize text data. Getting text ready for data work involves multiple stages of feature engineering and building a *pipeline*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "Now that we have our SMS corpus in nice and orderly vectorized format, we can finally select and train a machine learning algorithm that will (we hope) predict whether or not a given message is spam. Out of the many models we can choose from, we'll be using a naive Bayes classifier. It's a probabilistic classifier which has been used with great success for decades, especially in text classification tasks. It still remains competitive even against more modern methods, such as support vector machines.\n",
    "\n",
    "What seems a bit odd with this course is that we'll be passing our tfidf matrix to the naive Bayes classifier, which isn't how I've other guides do it. According to the official docs:\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with\n",
    "discrete features (e.g., word counts for text classification). The\n",
    "multinomial distribution normally requires integer feature counts. **However,\n",
    "in practice, fractional counts such as tf-idf may also work.**\n",
    "\n",
    "That's not exactly a ringing endorsement of the method, but YOLO, I guess.\n",
    "\n",
    "The number of dataframes, matrices, and whatnot are getting out of hand, so I'll review what we're working with.\n",
    "\n",
    "| name | type | description |\n",
    "| ---- | ---- | ----------- |\n",
    "| `sms` | `DataFrame` | A corpus of text messages with the features, `label`, `message`, and `length` |\n",
    "| `process` | `function` | A function for turning SMS messages into tokens without stopwords|\n",
    "| `bag_transformer` | `CountVectorizer` | A tool for converting a collection of documents into a matrix of token counts. After fitting a corpus, it contains a vocabulary of all the words in that corpus. |\n",
    "| `sample_sms` | `str` | A single message taken from row index `42` of the `sms` dataframe. |\n",
    "| `sample_bag` | `scr_matrix` | A bag of words made from `sample_sms` having been put through `bag_transformer.transform()` |\n",
    "| `sms_bag` | `scr_matrix` | The bag of words for the entire `sms` dataset having been put through `bag_transformer.transform()` |\n",
    "| `tfidf_transformer` | `TfidfTransformer` | A tool for converting a count matrix (such as a bag of words) into a normalized tf or tf-idf representation. |\n",
    "| `sample_tfidf` | `scr_matrix` | A tf-idf representation of `sample_sms` |\n",
    "| `sms_tfidf` | `scr_matrix` | A tf-idf representation of `sms_bag` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "spam_detection_model = MultinomialNB().fit(sms_tfidf, sms['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `spam_detection_model` should have everything necessary to detect speam messages, so long as the messages passed to it are in the form a matrices that are a tf-idf representation.\n",
    "\n",
    "Let's test it on a single message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted spam, and the real label is spam\n"
     ]
    }
   ],
   "source": [
    "sample_prediction = spam_detection_model.predict(sample_tfidf)[0]\n",
    "sample_label = sms['label'].iloc[42]\n",
    "print(f'Model predicted {sample_prediction}, and the real label is {sample_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! It's barely more scrutable than magic to me, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "No to put the entire dataset through the model and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham,ham,spam,ham,ham,ham,ham,ham,spam,spam,ham,spam,spam,ham,ham,spam,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,spam,ham,ham,spam,spam,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,spam,ham,ham,ham,spam,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,spam,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,spam,ham,ham,spam,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,spam,spam,ham,ham,ham,ham,ham,ham,ham,ham,spam,spam,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,spam,ham,spam,ham,spam,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,spam,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham,ham\n"
     ]
    }
   ],
   "source": [
    "sms_predictions = spam_detection_model.predict(sms_tfidf)\n",
    "print(','.join(list(p for p in sms_predictions)[:500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn's classification report, which we have used in many other lessons, can be utilized here. It will return the precision, recall, f1-score, and a support column. You've really got to understand those terms.\n",
    "\n",
    "**Precision** is the ratio of true positives to all positives (both true and false). It is the measure of many selected items are relevant.\n",
    "\n",
    "**Recall** is the ratio of true positives to all relevant elements (all possible true positives). It is a measure of how many relevant items were selected.\n",
    "\n",
    "Both precision and recall ought to be maximized towards 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      1.00      0.99      4825\n",
      "       spam       1.00      0.84      0.92       747\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(sms['label'], sms_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to get out of the way, and which you might have already noticed, is that we are using the exact same data to train and to test a model. This would **never** be acceptable in a real-world scenario. Putting that aside, let's see what we've got here.\n",
    "\n",
    "Overall the model looks to perform extremely well. For what amounted to a few hours' work we are able to classify spam and ham messages almost perfectly. But only almost.\n",
    "\n",
    "The **precision** for ham is 0.98, which means a few spam messages were erroniously classified as ham. The **precision** for spam, on the other hand, is 1.00. That means no ham messages were accidentally tossed into the spam folder.\n",
    "\n",
    "As for **recall**, the score for ham is 1.00, meaning that all ham was labels correctly for what it was. Spam, however, has a **recall** score of 0.84, which means that not all spam messages were correctly labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "OK, you know that we should have done this step earlier, but whatever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 1115, 4457, 1115)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sms_train, sms_test, label_train, label_test = \\\n",
    "    train_test_split(sms['message'], sms['label'], test_size=0.2)\n",
    "\n",
    "lengths = (len(x) for x in (sms_train, sms_test, label_train, label_test))\n",
    "print(tuple(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Data Pipeline\n",
    "\n",
    "Once we have a plan for our data, we can utilize Scikit-learn's pipeline abilites to store a workflow. With this we can set up ahead of time the transformations that we want to use on data that we will get in the future. Pipeline objects use the same interface as any other of Scikit-learn's estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.97      1.00      0.98       989\n",
      "       spam       0.99      0.72      0.83       126\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bag', CountVectorizer(analyzer=process)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(sms_train, label_train)\n",
    "predictions = pipeline.predict(sms_test)\n",
    "\n",
    "print(classification_report(label_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
